{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_0gBIAb4Y4z"
      },
      "outputs": [],
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame('https://www.youtube.com/embed/38aMTXY2usU', width=560, height=315)\n",
        "\n",
        "\n",
        "\n",
        "!pip install \"jedi>=0.16\" \"cryptography>=41.0.5,<44\"\n",
        "\n",
        "!pip install langchain langchain-core arxiv langchain_community docx2txt pypdf langchain_chroma sentence_transformers langchain-together unstructured together\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.environ.get(\"TOGETHER_API_KEY\"):\n",
        "  os.environ[\"TOGETHER_API_KEY\"] = getpass.getpass(\"Enter API key for Together AI: \")\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\", model_provider=\"together\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-course\""
      ],
      "metadata": {
        "id": "ovNDyP7u5yiN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "hKgLJLxU51fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DaylPC4w9gkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kf3kgiJA-mSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jzVW8_so-mQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import uuid\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "from google.colab import files  # For Colab file upload\n",
        "import arxiv\n",
        "import time\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_together import ChatTogether, TogetherEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# Environment setup\n",
        "os.environ[\"TOGETHERAI_API_KEY\"] = \"024254d1fcd8eee3a12258e40260345e8411c8afcec22c65bacar46f40aed1c60904\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-course\"\n",
        "\n",
        "# Database setup\n",
        "DB_NAME = \"rag_app.db\"\n",
        "\n",
        "def get_db_connection():\n",
        "    conn = sqlite3.connect(DB_NAME)\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    return conn\n",
        "\n",
        "def create_application_logs():\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
        "                    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                     session_id TEXT,\n",
        "                     user_query TEXT,\n",
        "                     gpt_response TEXT,\n",
        "                     model TEXT,\n",
        "                     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
        "    conn.close()\n",
        "\n",
        "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
        "                 (session_id, user_query, gpt_response, model))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def get_chat_history(session_id):\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
        "    messages = []\n",
        "    for row in cursor.fetchall():\n",
        "        messages.extend([\n",
        "            {\"role\": \"human\", \"content\": row['user_query']},\n",
        "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
        "        ])\n",
        "    conn.close()\n",
        "    return messages\n",
        "\n",
        "# Initialize model and embeddings\n",
        "model = ChatTogether(model=\"meta-llama/Llama-3-70b-chat-hf\")\n",
        "embedding_function = TogetherEmbeddings(model=\"togethercomputer/m2-bert-80M-8k-retrieval\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "# Global variables\n",
        "current_paper = None\n",
        "vectorstore = None\n",
        "rag_chain = None\n",
        "last_arxiv_papers = []  # To store arXiv search results with metadata\n",
        "last_input_time = time.time()  # Track the last user input time\n",
        "pdf_upload_mode = False  # Flag to track PDF upload mode\n",
        "\n",
        "def process_uploaded_pdf(file_path: str) -> str:\n",
        "    \"\"\"Process a PDF uploaded via Colab.\"\"\"\n",
        "    global current_paper, vectorstore\n",
        "\n",
        "    if current_paper:\n",
        "        return \"Error: Only one PDF can be processed at a time. Clear the current PDF with 'clear pdf'.\"\n",
        "\n",
        "    if not os.path.exists(file_path) or not file_path.lower().endswith('.pdf'):\n",
        "        return \"Error: Invalid or missing PDF file. Please upload a valid PDF.\"\n",
        "\n",
        "    try:\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        documents = loader.load()\n",
        "        if not documents:\n",
        "            return \"Error: Failed to load the PDF. Please upload a valid PDF.\"\n",
        "\n",
        "        splits = text_splitter.split_documents(documents)\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            collection_name=\"research_paper\",\n",
        "            documents=splits,\n",
        "            embedding=embedding_function,\n",
        "            persist_directory=\"./chroma_db\"\n",
        "        )\n",
        "        current_paper = {\"source\": file_path, \"processed\": True}  # Mark as processed immediately\n",
        "        setup_rag_chain()\n",
        "        global pdf_upload_mode\n",
        "        pdf_upload_mode = False  # Reset mode after successful upload\n",
        "        return f\"PDF uploaded successfully! Loaded {len(documents)} page(s) and split into {len(splits)} chunks. What would you like to know about it?\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: Failed to process PDF - {str(e)}. Please upload a valid PDF.\"\n",
        "\n",
        "def fetch_arxiv_papers(query: str, max_results: int = 3) -> str:\n",
        "    \"\"\"Fetch recent papers from arXiv using Client.results().\"\"\"\n",
        "    global current_paper, last_arxiv_papers\n",
        "\n",
        "    if current_paper and \"processed\" in current_paper:\n",
        "        return \"Error: Clear the current paper or PDF before fetching new papers from arXiv.\"\n",
        "\n",
        "    try:\n",
        "        client = arxiv.Client()\n",
        "        search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.SubmittedDate)\n",
        "        papers = list(client.results(search))\n",
        "        last_arxiv_papers = papers  # Store full paper objects for later use\n",
        "\n",
        "        if not papers:\n",
        "            return \"No arXiv results found for your query.\"\n",
        "\n",
        "        response = \"Here are some recent papers from arXiv:\\n\"\n",
        "        for i, paper in enumerate(papers, 1):\n",
        "            authors = \", \".join([author.name for author in paper.authors])\n",
        "            arxiv_link = paper.entry_id\n",
        "            response += f\"\"\"\n",
        "            {i}. **{paper.title}**\n",
        "               - **Abstract**: {paper.summary[:200]}...\n",
        "               - **DOI**: {paper.doi or 'Not found'}\n",
        "               - **Published**: {paper.published.year}\n",
        "               - **Authors**: {authors}\n",
        "               - **arXiv Link**: {arxiv_link}\n",
        "               - Select this paper by saying 'select paper {i}'\n",
        "            \"\"\"\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching arXiv papers: {str(e)}\"\n",
        "\n",
        "def select_arxiv_paper(paper_number: int) -> str:\n",
        "    \"\"\"Select a paper and display its details without immediate processing.\"\"\"\n",
        "    global current_paper, last_arxiv_papers\n",
        "\n",
        "    try:\n",
        "        if not last_arxiv_papers or paper_number < 1 or paper_number > len(last_arxiv_papers):\n",
        "            return \"Error: Invalid paper selection.\"\n",
        "\n",
        "        paper = last_arxiv_papers[paper_number - 1]\n",
        "        current_paper = {\"source\": paper.entry_id, \"title\": paper.title, \"metadata\": paper}\n",
        "\n",
        "        authors_str = \", \".join([author.name for author in paper.authors])\n",
        "        arxiv_link = paper.entry_id\n",
        "\n",
        "        response = f\"\"\"\n",
        "        **Selected Paper #{paper_number}: {paper.title}**\n",
        "        - **Abstract**: {paper.summary[:200]}...\n",
        "        - **DOI**: {paper.doi or 'Not found'}\n",
        "        - **Publish Year**: {paper.published.year}\n",
        "        - **Authors**: {authors_str}\n",
        "        - **arXiv Link**: {arxiv_link}\n",
        "\n",
        "        This paper has been selected! What would you like to know about it? (Note: Content will be processed on your first question.)\n",
        "        \"\"\"\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        return f\"Error selecting paper: {str(e)}\"\n",
        "\n",
        "def process_selected_paper(paper) -> bool:\n",
        "    \"\"\"Process the selected paper's PDF into the vector store.\"\"\"\n",
        "    global vectorstore, current_paper\n",
        "    try:\n",
        "        paper.download_pdf(filename=f\"temp_paper.pdf\")\n",
        "        file_path = f\"/content/temp_paper.pdf\"\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        documents = loader.load()\n",
        "        if not documents:\n",
        "            return False\n",
        "\n",
        "        splits = text_splitter.split_documents(documents)\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            collection_name=\"research_paper\",\n",
        "            documents=splits,\n",
        "            embedding=embedding_function,\n",
        "            persist_directory=\"./chroma_db\"\n",
        "        )\n",
        "        current_paper[\"processed\"] = True\n",
        "        setup_rag_chain()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing paper: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def setup_rag_chain():\n",
        "    \"\"\"Set up the RAG chain if vectorstore exists.\"\"\"\n",
        "    global vectorstore, rag_chain\n",
        "    if vectorstore and not rag_chain:\n",
        "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "        contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"Given a chat history and the latest user question, formulate a standalone question.\"),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", \"{input}\")\n",
        "        ])\n",
        "        qa_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.\"),\n",
        "            (\"system\", \"Context: {context}\"),\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "            (\"human\", \"{input}\")\n",
        "        ])\n",
        "        history_aware_retriever = create_history_aware_retriever(model, retriever, contextualize_q_prompt)\n",
        "        question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
        "        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "def handle_input(user_input: str, session_id: str, chat_history: List) -> str:\n",
        "    \"\"\"Handle user input and maintain conversation flow with exit intent detection.\"\"\"\n",
        "    global current_paper, vectorstore, rag_chain, last_arxiv_papers, last_input_time, pdf_upload_mode\n",
        "    last_input_time = time.time()  # Update the last input time\n",
        "\n",
        "    # Detect exit intent\n",
        "    if any(phrase in user_input.lower() for phrase in [\"exit\", \"bye\", \"quit\", \"end\"]) or \\\n",
        "       (\"no\" in user_input.lower() and any(exit_phrase in chat_history[-1][\"content\"].lower() for exit_phrase in [\"exit\", \"bye\", \"quit\", \"end\"])):\n",
        "        confirm = input(\"Are you sure you want to exit? (yes/no): \")\n",
        "        if confirm.lower() == \"yes\":\n",
        "            return \"Goodbye!\"\n",
        "        elif confirm.lower() == \"no\":\n",
        "            return \"Exit cancelled. How can I assist you further?\"\n",
        "        else:\n",
        "            return \"Please enter 'yes' or 'no' to confirm.\"\n",
        "\n",
        "    # Handle PDF upload mode\n",
        "    if user_input.lower() == \"switch to pdf\":\n",
        "        pdf_upload_mode = True\n",
        "        return \"PDF upload mode activated. Enter 'upload pdf' to open the file dialog, or press Enter to upload.\"\n",
        "\n",
        "    if pdf_upload_mode and user_input.lower() == \"upload pdf\":\n",
        "        uploaded = files.upload()\n",
        "        if uploaded:\n",
        "            file_path = f\"/content/{list(uploaded.keys())[0]}\"\n",
        "            if file_path.lower().endswith('.pdf'):\n",
        "                response = process_uploaded_pdf(file_path)\n",
        "                if \"Error\" not in response:\n",
        "                    insert_application_logs(session_id, \"uploaded PDF\", response, \"meta-llama/Llama-3-70b-chat-hf\")\n",
        "                    chat_history = get_chat_history(session_id)\n",
        "                pdf_upload_mode = False  # Reset mode after upload\n",
        "                return response\n",
        "            else:\n",
        "                pdf_upload_mode = False  # Reset mode on error\n",
        "                return \"Error: Only PDF files are supported. Please upload a valid PDF.\"\n",
        "        else:\n",
        "            return \"No file uploaded. Please try again or enter 'switch to pdf' to restart.\"\n",
        "        pdf_upload_mode = False  # Reset mode if upload fails or is cancelled\n",
        "\n",
        "    # Check if input is empty or handle other cases\n",
        "    if not user_input or user_input.strip() == \"\":\n",
        "        if pdf_upload_mode:\n",
        "            return \"Please enter 'upload pdf' to open the file dialog.\"\n",
        "        elif current_paper and \"processed\" in current_paper:\n",
        "            return \"PDF is already uploaded. Enter a query (e.g., 'summary') or use 'clear pdf' to start over.\"\n",
        "        return \"Please enter text, use 'switch to pdf' to upload a PDF, or use 'clear pdf' to reset.\"\n",
        "\n",
        "    if user_input.lower() == \"switch to text\":\n",
        "        pdf_upload_mode = False\n",
        "        return \"Text input mode activated. Please enter your text or command.\"\n",
        "\n",
        "    # Handle commands and queries\n",
        "    if user_input.lower().startswith(\"select paper \"):\n",
        "        try:\n",
        "            paper_num = int(user_input.split(\"select paper \")[1].strip())\n",
        "            response = select_arxiv_paper(paper_num)\n",
        "            if \"Error\" not in response:\n",
        "                insert_application_logs(session_id, user_input, response, \"meta-llama/Llama-3-70b-chat-hf\")\n",
        "                chat_history = get_chat_history(session_id)\n",
        "            return response\n",
        "        except ValueError:\n",
        "            return \"Error: Please provide a valid paper number after 'select paper'.\"\n",
        "\n",
        "    elif user_input.lower() == \"clear pdf\":\n",
        "        current_paper = None\n",
        "        vectorstore = None\n",
        "        rag_chain = None\n",
        "        last_arxiv_papers.clear()\n",
        "        pdf_upload_mode = False\n",
        "        response = \"Current paper or PDF cleared. You can upload a new one or enter a topic.\"\n",
        "        insert_application_logs(session_id, user_input, response, \"meta-llama/Llama-3-70b-chat-hf\")\n",
        "        chat_history = get_chat_history(session_id)\n",
        "        return response\n",
        "\n",
        "    # Process uploaded PDF or arXiv paper if available\n",
        "    elif current_paper and \"processed\" in current_paper:\n",
        "        response = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})[\"answer\"]\n",
        "        insert_application_logs(session_id, user_input, response, \"meta-llama/Llama-3-70b-chat-hf\")\n",
        "        chat_history = get_chat_history(session_id)\n",
        "        return response\n",
        "    elif current_paper and \"metadata\" in current_paper and \"processed\" not in current_paper:\n",
        "        if not process_selected_paper(current_paper[\"metadata\"]):\n",
        "            return \"Error: Failed to process the selected paper. Please try again or select another paper.\"\n",
        "        response = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})[\"answer\"]\n",
        "        insert_application_logs(session_id, user_input, response, \"meta-llama/Llama-3-70b-chat-hf\")\n",
        "        chat_history = get_chat_history(session_id)\n",
        "        return response\n",
        "\n",
        "    # Default to arXiv search if no PDF or processed paper\n",
        "    else:\n",
        "        response = fetch_arxiv_papers(user_input)\n",
        "        return response\n",
        "\n",
        "# Example usage in Colab\n",
        "create_application_logs()\n",
        "session_id = str(uuid.uuid4())\n",
        "chat_history = get_chat_history(session_id)\n",
        "last_input_time = time.time()  # Initialize last input time\n",
        "\n",
        "# Interactive loop with 10-minute timeout and exit intent detection\n",
        "print(\"Start by entering text or uploading a PDF. Use 'switch to pdf' to upload a PDF.\")\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"Human: \")\n",
        "        response = handle_input(user_input, session_id, chat_history)\n",
        "        if response == \"Goodbye!\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        print(f\"AI: {response}\\n\")\n",
        "\n",
        "        # Check for 10-minute inactivity\n",
        "        current_time = time.time()\n",
        "        if current_time - last_input_time > 600:  # 600 seconds = 10 minutes\n",
        "            print(\"No input received for 10 minutes. Exiting...\")\n",
        "            break\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\\n\")\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "eYfLFJpt-mOY",
        "outputId": "9f22198b-9f55-4de2-f7d5-ba428c37507e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start by entering text or uploading a PDF. Use 'switch to pdf' to upload a PDF.\n",
            "Human: switch to pdf\n",
            "AI: PDF upload mode activated. Enter 'upload pdf' to open the file dialog, or press Enter to upload.\n",
            "\n",
            "Human: upload pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5c90d721-21e4-41c7-9959-9bdbb27ab689\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5c90d721-21e4-41c7-9959-9bdbb27ab689\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving RBI Grade B 2024 Phase 2 - Memory Based Paper.pdf to RBI Grade B 2024 Phase 2 - Memory Based Paper.pdf\n",
            "AI: PDF uploaded successfully! Loaded 75 page(s) and split into 209 chunks. What would you like to know about it?\n",
            "\n",
            "Human: summary in 3 line.\n",
            "AI: Here is a summary in 3 lines:\n",
            "\n",
            "In aiohttp, you can set a timeout for the entire session or for individual requests using the `ClientTimeout` object. If the request is not successful within the specified time, an `asyncio.TimeoutError` is raised. The `asyncio.gather` method can be used to cleanly handle multiple awaitables with timeouts.\n",
            "\n",
            "Human: want to exit\n",
            "Are you sure you want to exit? (yes/no): yes\n",
            "Goodbye!\n"
          ]
        }
      ]
    }
  ]
}